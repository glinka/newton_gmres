\documentclass[11pt]{article}
%\renewcommand\thesubsection{\thesection.\alph{subsection}}
\usepackage{graphicx, subcaption, amsfonts, amsmath, amsthm, empheq}
\newtheorem*{thm:jnf}{Jordan normal form for square matrices}
%% some new commands I have no idea how they work
\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}
\newlength\dlf
\newcommand\alignedbox[2]{
  % Argument #1 = before & if there were no box (lhs)
  % Argument #2 = after & if there were no box (rhs)
  &  % Alignment sign of the line
  {
    \settowidth\dlf{$\displaystyle #1$}  
    % The width of \dlf is the width of the lhs, with a displaystyle font
    \addtolength\dlf{\fboxsep+\fboxrule}  
    % Add to it the distance to the box, and the width of the line of the box
    \hspace{-\dlf}  
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
    \boxed{#1 #2}
    % Put a box around lhs and rhs
  }
}
%% end new commands I have no idea how they work
\newcommand{\K}[1]{\mathcal{K}^{#1}}
\newcommand{\Kk}{\mathcal{K}^k}
\newcommand{\Forall}{\; \forall \;}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\captionsetup{labelformat=empty,labelsep=none}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\setlength\parindent{0pt}
\graphicspath{ {./figs/} }
\pagestyle{plain}
\begin{document}
\title{\vspace{-10mm}Newton-GMRES Overview}
\author{Alexander Holiday}
\maketitle

\section{Introduction}
Newton-GMRES is the synthesis of two separate numerical methods: the Newton-Rhapson fixed point method and the GMRES method for the solution of linear equations. We begin with a fairly detailed overview of both of these components.

\subsection{Newton-Rhapson (NR)}

When numerically solving the equation

\begin{align}
\label{eqn:F}
F(x) = 0 ; \; F: \mathbb{R}^N \rightarrow \mathbb{R}^N
\end{align}

some form of the Newton-Rhapson method is often used. The motivation behind this iterative procedure follows from a Taylor expansion of $F(x)$ around some starting point $x_0$. For ease of exposition, we examine $N=1$.

\begin{align*}
F(x_0 + \delta x) = F(x_0) + F'(x_0) \delta x + \mathcal{O}(\delta x^2)
\end{align*}

If we assume our next evaluation is close to the solution, we may drop the left-hand-side and find that

\begin{align*}
x_1 = x_0 - \frac{F(x_0)}{F'(x_0)} \\
\end{align*}

with the understanding that $\delta x = x_1 - x_0$. This suggests an iterative procedure for finding the solution to (\ref{eqn:F}), whereby 

\begin{align*}
x_{k+1} = x_k - \frac{F(x_k)}{F'(x_k)} \\
\end{align*}

starting from some $x_0$, and stopping, perhaps, when $F(x_k) < \epsilon$ where $\epsilon$ is some pre-specified tolerance. NR is appealing both for its simplicity, and because it can be shown to converge quadratically with a sufficiently good initial guess. We will show this now.

\subsection{NR Convergence}

First we must address a few preliminaries. We call a function $G(x), \; G: \Omega \rightarrow \mathbb{R}^M$ with $\Omega \subset \mathbb{R}^N$ Lipschitz continuous with Lipschitz constant $\gamma$ if

\begin{align*}
  \| G(x) - G(y) \| \leq \gamma \| x - y \| \Forall x, y \in \Omega
\end{align*}

If we find that $\gamma < 1$, then we call $G(x)$ a contraction mapping, as each application of $G$ to a pair of points decreases their distance from one another. If we define $x_{n+1} = G(x_n)$ for the contraction mapping $G$, then the series $\{x_i\}_{i=1}^\infty$ will converge to a unique point $x^*$. To see this, we first show that the series is a Cauchy sequence

\begin{align*}
  \| x_{n+1} - x_n \| = \| G(x_n) - G(x_{n-1}) \| \leq \gamma \| x_n - x_{n-1} \| \leq \hdots \leq \gamma^n \| x_1 - x_0 \| \\
\end{align*}

so

\begin{align*}
  \| x_n - x_0 \| &= \| \sum \limits_{i=1}^n( x_i - x_{i-1}) \| \\
  &\leq \sum \limits_{i=1}^n \| x_i - x_{i-1} \| \\
  &\leq \| x_1 - x_0 \| \sum \limits_{i=0}^n \gamma^i  = \frac{\| x_1 - x_0 \|}{1 - \gamma} \\
\end{align*}

Thus, for any $n$ and $k$,

\begin{align*}
  \| x_n - x_k \| &= \| \sum \limits_{i=k+1}^n( x_i - x_{i-1}) \| \\
  &\leq \sum \limits_{i=k}^{n-1} \gamma ^{i} \| x_1 - x_0 \| = \frac{\| x_1 - x_0 \|}{1 - \gamma} \bigg( y^k - y^n \bigg) \\
\end{align*}

which shows that we can always choose $n$ and $k$ to be smaller than some $\epsilon$, and therefore $x_i$ is a Cauchy sequence, which will converge to some $x^*$. To see that this point is unique, consider the case of two fixed points $a$ and $b$. Then $\| G(a) - G(b) \| = \| a - b \|$ by definition of a fixed point, but also $\| G(a) - G(b) \leq \gamma \| a - b \|$. The only solution to these inequalities is $\| a - b \| = 0$, which requires $a = b$.

With this in mind, we'd like to show that NR, which defines $G(x_n) = x_n - \frac{F(x_n)}{F'(x_n)}$, becomes a contraction mapping close to the solution $x^*$. We'll denote by $B(r)$ the ball of radius $r$ around $x^*$, i.e.

\begin{align*}
  B(r) = \{ x: \| x - x^* \| < r \}
\end{align*}

We would like to show that, for some $B(\delta)$, if $x_0 \in B(\delta)$, then all $x_{i+1} = G(x_i)$ will also be in $B(\delta)$, and furthermore that $\| G(x) - G(y) \| \leq \gamma \| x - y \|$ with a positive $\gamma < 1$. \\

We now appeal to the fundamnetal theorem of calculus: given a function $F$ differentiable in $\Omega \in \mathbb{R}^N$, for all $x \in \Omega$ sufficiently close to some $x^* \in \Omega$

\begin{align*}
  F(x) - F(x^*) = \int \limits_0^1 F'(x^* + t(x - x^*)(x - x^*)dt
\end{align*}

Now if $F'(x) \leq M \Forall x \in \Omega$ then we find

\begin{align*}
  \frac{\|F(x) - F(x^*)\|}{\|x - x^*\|} \leq M
\end{align*}

Thus, while we want to show that $\frac{\| G(x) - G(y) \|}{\| x - y \|} \leq 1$, it will be sufficient to find some region in which $G'(x) < 1$. This is easily seen as

\begin{align*}
  G'(x) = \frac{F(x)F''(x)}{F'(x)^2}
\end{align*}

We know that $F(x^*) = 0$, and thus, provided $F'(x^*) \neq 0$, $G'(x^*) = 0$. Therefore, there will be some region $B(\delta)$ in which $|G'(x)| < 1$ implying also that $\frac{\| G(x) - G(y) \|}{\| x - y \|} < 1$. However, in order for $G$ to be a contraction mapping, we must still show that $G(x_n) \in B(\delta) \Forall x_0 \in B(\delta)$. \\

To accomplish this, we could either show that $\| G(x) - x^* \| < \delta$ or, more significantly, that if, with the definition $e_n = x_n - x^*$, $e_{n+1} \leq e_n$. We'll start with an attempt at the former. Close to $x^*$ we find

\begin{align*}
  \| e_{n+1} \| &= \| G(x) - x^* \| \\
  &= \| G(x^*) + G'(x^*) + \mathcal{O}(\delta x^2) - x^* \| \\
  &= \| \mathcal{O}(\delta x^2) \|
\end{align*}

while $\| e_n \| = \| \delta x \|$.  We must examine $G''(x)$ to ensure nothing terrible happens to it that might alter our $\mathcal{O}(\delta x^2)$

\begin{align*}
  G''(x) = \frac{F''(x)}{F'(x)} + \frac{F(x)F'''(x)}{F'(x)^2} - 2 \frac{F(x)F''(x)^2}{F'(x)^3}
\end{align*}

Again we find that the assumption that $F'(x) \neq 0$ is crucial in our analysis. If this holds, both $G'(x)$ and $G''(x)$ will be well-behaved near $x^*$, and 

\begin{align*}
  \frac{\| e_{n+1} \|}{\| e_{n} \|} = \frac{\| \mathcal{O}(\delta x^2) \|}{\| \delta x \|} < 1
\end{align*}
 
for some $B(\epsilon)$. Thus we simply take the smaller of the two, $\epsilon$ or $\delta$ and we find that in this region $G(x)$ is indeed a contraction mapping that will converge to a unique fixed point $x^*$. We also recover the quadratic convergence we mentioned earlier, as

\begin{align*}
  \| e_{n+1} \| = \| \mathcal{O}(\delta x^2) \| \leq \| \mathcal{O}(\delta x) \|^2 = \| \mathcal{O}(e_n) \|^2
\end{align*}

Thus, near the fixed point, errors decrease quadratically. I realize this wasn't terribly rigorous, but I believe the main points are correct and could be cleaned up without too much effort. Now that we thoroughly understand the NR method, we turn to an overview of the second piece of the puzzle: a method called the generalized minimum residual.

\subsection{Generalized Minimum Residual (GMRES)}

Belonging to the class of Krylov-space methods, GMRES was the successor of conjugate gradient iterations, which was used to solve $Ax=b$ when $A$ was symmetric and positive-definite. In these Kryolv methods, the $k^{th}$ iterate seeks to minimize some error over the affine space

\begin{align*}
  x_0 + \mathcal{K}^k
\end{align*}

known as the $k^{th}$ Krylov subspace, in which $\mathcal{K}^k$ is

\begin{align*}
  \mathcal{K}^k = \textrm{span}(r_0, Ar_0, \hdots, A^{k-1}r_0)
\end{align*}

for $k \geq 1$. \\

At the $k^{th}$ iteration of GMRES, we seek

\begin{align*}
  x_k = \argmin \limits_{x \in x_0 + \Kk} \| b - Ax \|_2
\end{align*}

Suppose we have some orthogonal basis for $\Kk$, $V_k$. Then any $v \in \Kk$ can be written as $v = V_k y$, and we can rewrite our minimization problem as

\begin{align*}
  y_k &= \argmin \limits_{y \in \mathbb{R}^k} \| b - A(x_0 + V_k y \|_2 \\
  &= \argmin \limits_{y \in \mathbb{R}^k} \| r_0 - A V_k y\|_2
\end{align*}

where $r = b - Ax$. Given that the columns of $V_k$, $v_k^l$, are orthogonal, this becomes a standard least-squares minimization problem. However, we must, at every step, calculate $A V_k$, which can become very expensive when $n$ and $k$ are large. What would be ideal is to simply be able to extend $A V_k$ into the new subspace via the addition of another column at each iteration, instead of re-multiplying the full product each time. In fact, by using Gram-Schmidt orthogonalization to form an orthonormal basis for $\Kk$, we can do just that. This is known as the Arnoldi process, and it proceeds as follows \\

\textbf{Input:} $(x_0, b, A, k)$
\begin{enumerate}
\item Define $r_0 = b - A x_0$ and set $v_1 = \frac{r_0}{\|r_0\|}$
\item For $i = 2, \dots, k$

\begin{align*}
  v_{i+1} = \frac{A v_i - \sum \limits_{j=1}^i ((A v_i)^T v_j) v_j}{\| A v_i - \sum \limits_{j=1}^i ((A v_i)^T v_j) v_j \|_2}
\end{align*}

\end{enumerate}
\textbf{Output:} $V_k = \{v_1, v_2, \dots, v_k\}$ \\

Note that the resulting basis can be constructed in an additive fashion: calculating the $k+1^{th}$ vector doesn't alter the previous $k$. It may happen that the denominator in step 2 is zero. If this is the case, it can be shown that the solution $x = A^{-1}b$ lies in $x_0 + \K{k-1}$. \\

This is certainly useful, but the real magic has only just begun. Because the columns of $V_k$ are orthonormal, we know that \textbf{(why?)}

\begin{align*}
  AV_k = V_k H_k
\end{align*}

for some nonsingular $H_k \in \mathbb{R}^{k \times k}$. Let us define $r_0 = \beta V_k e_1$, then at the $k^{th}$ step we need

\begin{align*}
  y_k &= \argmin \limits_{y \in \mathbb{R}^k} \| r_0 - A V_k y\|_2 \\
  &= \argmin \limits_{y \in \mathbb{R}^k} \| V_k( \beta e_1 - H y)\|_2 \\
  &= \argmin \limits_{y \in \mathbb{R}^k} \| \beta e_1 - H y\|_2 \\
\end{align*}

Given that a stopping condition of $\| r_k \| < \epsilon$, we also see that $\| r_k \|_2 = \| \beta e_1 - H y\|_2$, so that we never actually need to calculate $x$ to find $b - Ax$, saving yet more time. Now let us define a new matrix $H_k \in \mathbb{R}^{k+1 \times k}$, with entries $h_{ij} = (Av_j)^Tv_i$. Then $H_k$ is called upper Hessenberg, as $h_{ij} = 0$ for $i > j + 1$. The Arnoldi process will give us a basis $V_k$ such that

\begin{align*}
  A V_k = V_{k+1} H_k
\end{align*}

This changes none of the preceeding results with $H$, but can lead to a more efficient algorithm. 

\textbf{Input:} $(x_0, b, A, \epsilon)$.
\begin{enumerate}
\item Set $r = b - Ax$, $v_1 = \frac{r}{\| r \|}$, $\rho = \| r \|$, $\beta = \rho$
\item While $\rho > \epsilon \| b \|$ do
  \begin{enumerate}
  \item for $j = 1, \dots, k$ \\
    $h_{jk} = (A v_k)^T v_j$
  \item $v_{k+1} = A v_k - \sum \limits_{j=1}^k h_{jk} v_j$
  \item $h_{k+1,k} = \| v_{k+1} \|$
  \item $ v_{k+1} = \frac{v_{k+1}}{\| v_{k+1} \|}$
  \item $e_1 = (1, 0,\dots, 0)^T \in \mathbb{R}^{k+1}$ \\
    Minimize $\| \beta e_1 - H_k y \|$ over $\mathbb{R}^{k+1}$ to find $y_k$
  \item $\rho = \| \beta e_1 - H_k y_k \|$
\end{enumerate}
\item $x_k = x_0 + V_k y_k$
\end{enumerate}
\textbf{Output:} $x_k$

This algorithm is fairly efficient, but we can do better. For one thing, the columns of $V_k$ may lose their orthogonality as the method progresses. There are two possible solutions, the first is to use a modified Gram-Schmidt procedure in step 2b

\begin{align*}
  v_{k+1} &= A v_k \\
  \textrm{for} j &= 1, \dots, k \\
  v_{k+1} &= v_{k+1} - (v_{k+1}^Tv_j)v_j \\
\end{align*}

While in infinite precision, both approaches are equivalent, this adjusted procedure is more likely to retain numerical orthogonality. \\

Additionally, we can re-orthogonalize \textbf{(what does this mean?)} if, say $\delta \| v_{k+1} \| = 0$ to numerical precision. If this condition held, we might suspect that we had lost information in the construction of the $k^{th}$ vector, and would need to re-form it via

\begin{itemize}
\renewcommand{\labelitemi}{$\circ$}
\item For $j = 1, \dots, k$ \\
  $h_{tmp} = v_{k+1}^T v_j$ \\
  $h_{jk} = h_{jk} + h_{tmp}$ \\
  $v_{k+1} = v_{k+1} - h_{tmp}v_j$ \\
\end{itemize}

and continuing as normal. \\

However, the real magic remains. Before reaching our final algorithm, we introduce the Givens rotation matrix

\begin{align*}
  G = \begin{pmatrix} c & -s \\ s & c \end{pmatrix}
\end{align*}

where $c = \cos(\theta)$ and $s = \sin(\theta)$. We see that this orthogonal matrix will rotate a vector $\begin{pmatrix} c \\ -s \end{pmatrix}$ to be aligned with the $x_1$-axis, as

\begin{align*}
  \begin{pmatrix} c & -s \\ s & c \end{pmatrix} \begin{pmatrix} c \\ -s \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}
\end{align*}

We can similarly define an $N \times N$ Givens matrix as

\begin{align*}
  G_i^N = \begin{pmatrix} 1 & 0 & & \dots & & & 0 \\ 0 & \ddots & \ddots & & & & \\ & \ddots & c & -s & & & \\ \vdots & & s & c & 0 & & 0 \\ & & & 0 & 1 & \ddots & \\ & & & & \ddots & \ddots & 0 \\ 0 & & & \dots & & 0 & 1 \end{pmatrix}
\end{align*}

where $G_{ii} = c$. This could eliminate the $-s$ entry in some vector $v = \{v(1), \dots , c, -s, \dots, v(N)\}^T$. In general, Givens matrices are useful in removing single non-zero entries on the path towards triangularization, and are especially helpful when applied to upper Hessenberg matrices. \\

We start by defining $G_1$ with $c_1 = \frac{h_{11}}{\sqrt{h_{11}^2 + h_{21}^2}}$ and $s_1 = -\frac{h_{21}}{\sqrt{h_{11}^2 + h_{21}^2}}$. Now $G_1 H$ would replace the first column of $H$ with a single nonzero entry $h_{11}$. If we continue with $G_2$ for which $c_2 = \frac{h_{22}}{\sqrt{h_{22}^2 + h_{32}^2}}$ and $s_2 = -\frac{h_{32}}{\sqrt{h_{22}^2 + h_{32}^2}}$ we eliminate $H_{32}$ while leaving the first column unchanged! Thus we have

\begin{align*}
  G_1 G_2 \dots G_k H = QH = R
\end{align*}

where $R$ is an upper-triangular $k+1 \times k$ matrix. Defining $g = \beta Q e_1$

\begin{align*}
  \| \beta e_1 - H_k y \| = \| Q(\beta e_1 - H_k y) \| = \| g - R y \|
\end{align*}

Again, $R$ is the upper-triangular factor of the QR factorization of $H_k$. It would be a straightforward application of these ideas to calculate $R$ at each step and solve the resulting optimization problem, but in fact, just as with the Arnoldi process, we can incrementally expand $R$ as the method progresses. After we've calculated the next orthogonal basis column of $V_{k+1}$ and add the corresponding $k+1^{th}$ column, denoted $h_{k+2}$ as it contains $k+2$ elements, to $H_k$, we can update $Q_k$ and $R_k$ by first applying the existing $Q_k$ to $h_{k+1}$, then finding the $G_{k+1}$ Givens matrix required to eliminate the $(k + 2)$nd element of $Q_k h_{k+2}$. Then $Q_{k+1} = G_{k+1} Q_k$ and we augment $R_k$ with $Q_{k+1} h_{k+2}$. At long last, the final algorithm we use to perform GMRES in the solution of $Ax = b$ looks like

\textbf{Input:} $(x_0, b, A, \epsilon, kmax)$
\begin{enumerate}
\item $r_0 = b - A x_0$, $v_1 = \frac{r_0}{\| r_0 \|}$, $\beta = \rho = \| r_0 \|$ \\
  $k = 0$, $g = \rho (1, 0, \dots, 0)^T \in \mathbb{R}^{kmax + 1}$
\item While $\rho > \epsilon \| b \|$ and $k < kmax$
  \begin{enumerate}
  \item $k = k + 1$
  \item $v_{k+1} = A v_k$ \\
    for $j = 1, \dots, k$ \\
    \begin{enumerate}
    \item $h_{jk} = v_{k+1}^T v_j$
    \item $v_{k+1} = v_{k+1} - h_{jk} v_j$
    \end{enumerate}
  \item $h_{k+1,k} = \| v_{k+1} \|$
  \item Test for orthogonality, reorthogonalize if necessary
  \item $v_{k+1} = \frac{v_{k+1}}{\| v_{k+1} \|}$
  \item \begin{enumerate}
  \item If $k > 1$ apply $Q_{k-1}$ to the $k^{th}$ column of $H$
  \item $\nu = \sqrt{h_{kk}^2 + h_{k+1,k}^2}$
  \item $c_k = \frac{h_{kk}}{\nu}$, $s_k = -\frac{h_{k+1,k}}{\nu}$ \\
    $h_{kk} = c_k h_{kk} - s_k h_{k+1,k}$, $h_{k+1, k} = 0$
  \item $g = G_k(c_k, s_k)g$
  \end{enumerate}
  \item $\rho = |(g)_{k+1}|$
  \end{enumerate}
  \item Set $r_{ij} = h_{ij}$ for $1 \leq i$, $j \leq k$ \\
    Set $(w)_i = (g)_i$ for $1 \leq i \leq k$ \\
    Solve the upper triangular system $Ry_k = w$
  \item $x_k = x_0 + V_k y_k$
\end{enumerate}

\textbf{Output:} $x_k$
    

\end{document}
